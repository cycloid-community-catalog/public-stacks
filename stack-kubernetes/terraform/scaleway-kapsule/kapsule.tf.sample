
#
# Create a Kapsule cluster + node pools
#

# Config module is used to generate special datas shared with other modules like cluster_name
module "config" {
  #####################################
  # Do not modify the following lines #
  source = "./module-config"

  project  = var.project
  env      = var.env
  customer = var.customer

  #####################################

  cluster_name = var.cluster_name
}

module "kapsule" {
  #####################################
  # Do not modify the following lines #
  source = "./module-kapsule"

  project  = var.project
  env      = var.env
  customer = var.customer

  #####################################

  ###
  # General
  ###

  #. extra_tags (optional): []
  #+ Dict of extra tags to add on aws resources. format [ "foo=bar" ].
  extra_tags = []
  
  ###
  # Control plane
  ###

  #. cluster_version (optional): 1.19
  #+ Kapsule cluster version.
  cluster_version = "1.19"

  #. cni (optional): cilium
  #+ The Container Network Interface (CNI) for the Kubernetes cluster (either `cilium`, `calico`, `weave` or `flannel`).
  cni = "cilium"

  #. ingress (optional): nginx
  #+ The ingress controller to be deployed on the Kubernetes cluster (either `nginx`, `traefik`, `traefik2` or `none`).
  ingress = "nginx"

  #. enable_dashboard (optional): true
  #+ Enables the Kubernetes dashboard for the Kubernetes cluster.
  enable_dashboard = true

  #. feature_gates (optional): []
  #+ The list of feature gates to enable on the cluster.

  #. admission_plugins (optional): []
  #+ The list of admission plugins to enable on the cluster.

  #. auto_upgrade (optional): true
  #+ Set to `true` to enable Kubernetes patch version auto upgrades. Important: When enabling auto upgrades, the `cluster_version` variable take a minor version like x.y (ie 1.18).  
  auto_upgrade = true

  #. maintenance_window_day (optional): "tuesday"
  #+ The day of the auto upgrade maintenance window (`monday` to `sunday`, or `any`).
  maintenance_window_day =  "tuesday"

  #. maintenance_window_start_hour (optional): 5
  #+ "The start hour (UTC) of the 2-hour auto upgrade maintenance window (`0` to `23`)."
  maintenance_window_start_hour = 5

  ###
  # Required (should probably not be touched)
  ###

  cluster_name = module.config.cluster_name
  scw_region   = var.scw_region
  scw_zone     = local.scw_zone
}

# You can duplicate this module to create mutiple Kapsule node pools.
# Make sure to change the module name and the `node_pool_name` field accordingly.
module "node_pool" {
  #####################################
  # Do not modify the following lines #
  source = "./module-node-pool"

  project  = var.project
  env      = var.env
  customer = var.customer

  #####################################

  ###
  # General
  ###

  #. extra_tags (optional): []
  #+ Dict of extra tags to add on aws resources. format [ "foo=bar" ].
  extra_tags = []

  ###
  # Nodes
  ###

  #. node_pool_name (optional): standard
  #+ Node group given name.
  node_pool_name = "standard"

  #. node_type (optional): GP1-XS
  #+ Type of instance to use for node servers.
  node_type = "GP1-XS"

  #. node_count (optional): 1
  #+ Desired number of node servers.
  node_count = "1"

  #. enable_autoscaling (optional): false
  #+ Enables the autoscaling feature for this pool. Important: When enabled, an update of the size will not be taken into account.
  enable_autoscaling = false

  #. node_autoscaling_min_size (optional): 1
  #+ The minimum size of the pool, used by the autoscaling feature.
  node_autoscaling_min_size = 1

  #. node_autoscaling_max_size (optional): 10
  #+ The maximum size of the pool, used by the autoscaling feature.
  node_autoscaling_max_size = 10

  #. enable_autohealing (optional): true
  #+ Enables the autohealing feature for this pool.
  enable_autohealing = true

  #. container_runtime (optional): docker
  #+ The container runtime of the pool. Important: Updates to this field will recreate a new resource.

  #. placement_group_id (optional): ""
  #+ The placement group the nodes of the pool will be attached to. Important: Updates to this field will recreate a new resource.
  placement_group_id = ""

  ## To add placement group, create a separate file in the /config branch for the stack
  ## Containing the following resource
  # resource "scaleway_instance_placement_group" "($ environment $)" {
  #  name        = var.placement_group_name
  #  policy_type = "max_availability"
  #  policy_mode = "optional"
  #  zone        = local.scw_zone
  # }


  #. wait_for_pool_ready (optional): true
  #+ Whether to wait for the pool to be ready.

  ###
  # Required (should probably not be touched)
  ###

  cluster_id   = module.kapsule.cluster_id
  cluster_name = module.config.cluster_name
  scw_region   = var.scw_region
  scw_zone     = local.scw_zone
}
